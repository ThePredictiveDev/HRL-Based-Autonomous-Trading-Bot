{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05555912",
   "metadata": {},
   "source": [
    "# DQN Based Hierarchical RL Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf32382",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe24aa65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Input\n",
    "from keras.optimizers import Adam\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a601d1a",
   "metadata": {},
   "source": [
    "## Setting up the DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e23b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.95    # discount rate\n",
    "        self.epsilon = 1.0   # exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        # Neural Net for Deep Q-learning Model\n",
    "        model = Sequential()\n",
    "        model.add(Input(shape=(self.state_size,)))\n",
    "        model.add(Dense(24, activation='relu'))\n",
    "        model.add(Dense(24, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer=Adam(learning_rate=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        act_values = self.model.predict(state)\n",
    "        return np.argmax(act_values[0])   # returns action with the highest Q-value\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target = (reward + self.gamma *\n",
    "                          np.amax(self.model.predict(next_state)[0]))\n",
    "            target_f = self.model.predict(state)\n",
    "            target_f[0][action] = target\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650ca358",
   "metadata": {},
   "source": [
    "## Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441d272d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(tickers, daily_folder='data/processed', intraday_folder='data/processed_intra_day', synthetic_data_dir = 'data/synthetic_data/'):\n",
    "    intraday_data = {}\n",
    "\n",
    "    data_dir = 'data/processed/'\n",
    "    intra_data_dir = 'data/processed_intra_day/'\n",
    "    synthetic_data_dir = 'data/synthetic_data/'\n",
    "    processed_data = {}\n",
    "    synthetic_data = {}\n",
    " \n",
    "    for file in os.listdir(data_dir):\n",
    "        if file.endswith('_final.csv'):\n",
    "            ticker = file.replace('_final.csv', '')\n",
    "            df = pd.read_csv(os.path.join(data_dir, file))\n",
    "            df.dropna(inplace=True)  # Drop any rows with NaN values\n",
    "            processed_data[ticker] = df\n",
    "            \n",
    "    for file in os.listdir(intra_data_dir):\n",
    "        if file.endswith('_intraday_processed.csv'):\n",
    "            ticker = file.replace('_intraday_processed.csv', '')\n",
    "            df = pd.read_csv(os.path.join(intra_data_dir, file))\n",
    "            df.dropna(inplace=True)  # Drop any rows with NaN values\n",
    "            intraday_data[ticker] = df\n",
    "            \n",
    "    for ticker_folder in os.listdir(synthetic_data_dir):\n",
    "        ticker_folder_path = os.path.join(synthetic_data_dir, ticker_folder)\n",
    "        \n",
    "        if os.path.isdir(ticker_folder_path):\n",
    "            for file in os.listdir(ticker_folder_path):\n",
    "                if file.endswith('_synthetic.csv'):\n",
    "                    ticker = file.split('_')[0]\n",
    "                    file_path = os.path.join(ticker_folder_path, file)\n",
    "                    df.dropna(inplace=True)  # Drop any rows with NaN values\n",
    "                    synthetic_data[ticker] = pd.read_csv(file_path)\n",
    "\n",
    "    return processed_data, intraday_data, synthetic_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc2c53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "midcap_stocks = [\n",
    "    \"MSUMI.NS\", \"TORNTPOWER.NS\", \"GODREJPROP.NS\", \"SRF.NS\",\n",
    "    \"APLAPOLLO.NS\", \"TVSMOTOR.NS\", \"LTIM.NS\", \"PAGEIND.NS\",\n",
    "    \"AUROPHARMA.NS\", \"JINDALSTEL.NS\", \"BAJAJHLDNG.NS\", \"BATAINDIA.NS\",\n",
    "    \"BHEL.NS\", \"CANBK.NS\", \"CHOLAFIN.NS\", \"CUB.NS\", \"DALMIASUG.NS\",\n",
    "    \"ESCORTS.NS\", \"FEDERALBNK.NS\", \"FORTIS.NS\", \"GICRE.NS\",\n",
    "    \"GMRINFRA.NS\", \"GNFC.NS\", \"GODREJAGRO.NS\", \"GRASIM.NS\", \"HAVELLS.NS\",\n",
    "    \"HINDPETRO.NS\", \"INDHOTEL.NS\", \"JUBLFOOD.NS\", \"LICHSGFIN.NS\",\n",
    "    \"M&MFIN.NS\", \"MANAPPURAM.NS\", \"MRF.NS\", \"NATCOPHARM.NS\",\n",
    "    \"NCC.NS\", \"NMDC.NS\", \"OBEROIRLTY.NS\", \"PERSISTENT.NS\", \"PETRONET.NS\",\n",
    "    \"RAMCOCEM.NS\", \"RBLBANK.NS\", \"SAIL.NS\", \"SUNTV.NS\", \"TATACOMM.NS\",\n",
    "    \"TATAPOWER.NS\", \"THYROCARE.NS\", \"TORNTPHARM.NS\", \"TRENT.NS\", \"VOLTAS.NS\",\n",
    "    \"WHIRLPOOL.NS\", \"YESBANK.NS\", \"ZEEL.NS\", \"ZYDUSWELL.NS\",\n",
    "    \"ABBOTINDIA.NS\", \"ASHOKLEY.NS\", \"BALKRISIND.NS\", \"BEL.NS\", \"CONCOR.NS\",\n",
    "    \"CROMPTON.NS\", \"DEEPAKNTR.NS\", \"DIXON.NS\", \"EMAMILTD.NS\",\n",
    "    \"INDIAMART.NS\", \"IRCTC.NS\", \"JUBLPHARMA.NS\", \"LTTS.NS\", \"MFSL.NS\",\n",
    "    \"METROPOLIS.NS\", \"OBEROIRLTY.NS\", \"PIIND.NS\", \"POLYCAB.NS\", \"RECLTD.NS\",\n",
    "    \"SUPREMEIND.NS\", \"TATACONSUM.NS\", \"TV18BRDCST.NS\", \"VGUARD.NS\",\n",
    "    \"VBL.NS\", \"VINATIORGA.NS\", \"ZENSARTECH.NS\", \"IDFCFIRSTB.NS\",\n",
    "    \"SONACOMS.NS\", \"AMBUJACEM.NS\", \"GAIL.NS\", \"TATAELXSI.NS\", \"MAXHEALTH.NS\",\n",
    "    \"LALPATHLAB.NS\", \"JSWENERGY.NS\", \"AARTIIND.NS\", \"ADANIGREEN.NS\",\n",
    "    \"ABFRL.NS\", \"BANDHANBNK.NS\", \"BANKINDIA.NS\", \"BERGEPAINT.NS\", \"BOSCHLTD.NS\",\n",
    "    \"CUMMINSIND.NS\", \"DMART.NS\", \"GLENMARK.NS\", \"GUJGASLTD.NS\",\n",
    "    \"HAL.NS\", \"IIFLWAM.NS\", \"LICI.NS\", \"LUXIND.NS\", \"M&MFIN.NS\",\n",
    "    \"NAUKRI.NS\", \"PHOENIXLTD.NS\", \"RAJESHEXPO.NS\", \"SHREECEM.NS\",\n",
    "    \"TATACHEM.NS\", \"THERMAX.NS\", \"TTKPRESTIG.NS\", \"UJJIVANSFB.NS\", \"VAKRANGEE.NS\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90084676",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data, intraday_data, synthetic_data = load_data(midcap_stocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046d1214",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "processed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febedb4f",
   "metadata": {},
   "source": [
    "## Defining the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dde0a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TradingEnv(gym.Env):\n",
    "    def __init__(self, data, ticker_list):\n",
    "        super(TradingEnv, self).__init__()\n",
    "        self.data = data\n",
    "        self.ticker_list = ticker_list\n",
    "        self.current_step = 0\n",
    "        self.current_ticker = ticker_list[0]\n",
    "        self.initial_portfolio_value = 1000000  # Initial portfolio value (e.g., $1,000,000)\n",
    "        self.portfolio_value = self.initial_portfolio_value  # Start with the initial value\n",
    "        self.position = 0  # Number of shares currently held\n",
    "\n",
    "        # State space size depends on the number of features (e.g., OHLCV, indicators)\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(len(data[self.current_ticker].columns),), dtype=np.float32)\n",
    "        self.action_space = spaces.Discrete(3)  # 0 = Hold, 1 = Buy, 2 = Sell\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.current_ticker = np.random.choice(self.ticker_list)  # Randomly select a ticker\n",
    "        self.position = 0  # Reset position\n",
    "        self.portfolio_value = self.initial_portfolio_value  # Reset portfolio value\n",
    "        return self.data[self.current_ticker].iloc[self.current_step].values\n",
    "\n",
    "    def step(self, action):\n",
    "        done = False\n",
    "        reward = 0\n",
    "\n",
    "        # Market data for the current step\n",
    "        current_price = self.data[self.current_ticker].iloc[self.current_step]['Close']\n",
    "\n",
    "        # Implement trade logic and update the portfolio\n",
    "        if action == 1:  # Buy\n",
    "            reward = self._execute_buy(current_price)\n",
    "        elif action == 2:  # Sell\n",
    "            reward = self._execute_sell(current_price)\n",
    "        else:\n",
    "            reward = self._hold_position(current_price)\n",
    "\n",
    "        self.current_step += 1\n",
    "        if self.current_step >= len(self.data[self.current_ticker]):\n",
    "            done = True\n",
    "            self.current_step = len(self.data[self.current_ticker]) - 1  # Adjust to the last valid index\n",
    "\n",
    "        next_state = self.data[self.current_ticker].iloc[self.current_step].values\n",
    "        return next_state, reward, done, {}\n",
    "\n",
    "    def _execute_buy(self, current_price):\n",
    "        if self.position == 0:  # Buy only if not already holding\n",
    "            self.position = self.portfolio_value // current_price  # Buy as many shares as possible\n",
    "            self.portfolio_value -= self.position * current_price  # Update cash\n",
    "        reward = self._calculate_profit(current_price)\n",
    "        return reward\n",
    "\n",
    "    def _execute_sell(self, current_price):\n",
    "        if self.position > 0:  # Sell only if holding shares\n",
    "            self.portfolio_value += self.position * current_price  # Sell all shares\n",
    "            self.position = 0  # Reset position\n",
    "        reward = self._calculate_profit(current_price)\n",
    "        return reward\n",
    "\n",
    "    def _hold_position(self, current_price):\n",
    "        reward = self._calculate_profit(current_price)\n",
    "        return reward\n",
    "\n",
    "    def _calculate_profit(self, current_price):\n",
    "        # Calculate unrealized profit if still holding\n",
    "        if self.position > 0:\n",
    "            total_value = self.portfolio_value + self.position * current_price\n",
    "        else:\n",
    "            total_value = self.portfolio_value\n",
    "        return total_value - self.initial_portfolio_value  # Reward based on portfolio value increase\n",
    "    \n",
    "    def get_state(self):\n",
    "        \"\"\"\n",
    "        Returns the current state of the environment.\n",
    "        This includes market data, technical indicators, and current portfolio status.\n",
    "        \"\"\"\n",
    "        market_data = self.data[self.current_ticker].iloc[self.current_step]\n",
    "        state = {\n",
    "            'Open': market_data['Open'],\n",
    "            'High': market_data['High'],\n",
    "            'Low': market_data['Low'],\n",
    "            'Close': market_data['Close'],\n",
    "            'Volume': market_data['Volume'],\n",
    "            # Include other indicators as needed\n",
    "            'SMA_50': market_data.get('SMA_50', 0),  # Example for Simple Moving Average\n",
    "            'SMA_200': market_data.get('SMA_200', 0),\n",
    "            'RSI': market_data.get('RSI', 0),\n",
    "            'MACD': market_data.get('MACD', 0),\n",
    "            'MACD_Signal': market_data.get('MACD_Signal', 0),\n",
    "            'Bollinger_Upper': market_data.get('Bollinger_Upper', 0),\n",
    "            'Bollinger_Lower': market_data.get('Bollinger_Lower', 0),\n",
    "            # Add other indicators and state variables as necessary\n",
    "        }\n",
    "        return state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1309ac",
   "metadata": {},
   "source": [
    "## High Level Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2c21ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HighLevelAgent:\n",
    "    def __init__(self, env, total_budget=100000):\n",
    "        self.env = env\n",
    "        self.tickers = env.ticker_list\n",
    "        self.total_budget = total_budget\n",
    "        self.initial_portfolio_value = 1000000  # Initial portfolio value\n",
    "        self.portfolio_value = self.initial_portfolio_value\n",
    "        self.profit_target = 0.05  # 5% profit target\n",
    "        self.take_profit_threshold = 0.03  # Take profit at 3% gain\n",
    "\n",
    "    def choose_equities_and_allocate_budget(self):\n",
    "        # Select multiple equities\n",
    "        selected_tickers = self.select_tickers()\n",
    "\n",
    "        # Allocate budget based on Sharpe ratio\n",
    "        allocated_budgets = self.allocate_budget(selected_tickers)\n",
    "\n",
    "        # Create and manage mid- and low-level agents for each selected equity\n",
    "        agents = []\n",
    "        for ticker, budget in zip(selected_tickers, allocated_budgets):\n",
    "            mid_agent = MidLevelAgent(66, 20)\n",
    "            low_agent = LowLevelAgent(20, 3)\n",
    "            agents.append((ticker, budget, mid_agent, low_agent))\n",
    "\n",
    "        return agents\n",
    "\n",
    "    def select_tickers(self, num_tickers=3):\n",
    "        # Select `num_tickers` tickers based on the evaluation score\n",
    "        scores = {ticker: self.evaluate_ticker(ticker) for ticker in self.tickers}\n",
    "        sorted_tickers = sorted(scores, key=scores.get, reverse=True)\n",
    "        selected_tickers = sorted_tickers[:num_tickers]\n",
    "        return selected_tickers\n",
    "\n",
    "    def evaluate_ticker(self, ticker):\n",
    "        data = self.env.data[ticker]\n",
    "        returns = data['Close'].pct_change().dropna()\n",
    "        avg_return = returns.mean()\n",
    "        volatility = returns.std()\n",
    "        sharpe_ratio = avg_return / volatility\n",
    "        max_drawdown = (data['Close'].max() - data['Close'].min()) / data['Close'].max()\n",
    "\n",
    "        # Combine metrics into a single score\n",
    "        score = sharpe_ratio - max_drawdown\n",
    "        return score\n",
    "\n",
    "    def allocate_budget(self, selected_tickers):\n",
    "        # Calculate the Sharpe ratio for each selected ticker\n",
    "        sharpe_ratios = {ticker: self.calculate_sharpe_ratio(ticker) for ticker in selected_tickers}\n",
    "\n",
    "        # Total sum of all Sharpe ratios\n",
    "        total_sharpe = sum(sharpe_ratios.values())\n",
    "\n",
    "        # Allocate budget proportionally to the Sharpe ratio of each equity\n",
    "        allocated_budgets = [(sharpe_ratios[ticker] / total_sharpe) * self.total_budget for ticker in selected_tickers]\n",
    "\n",
    "        return allocated_budgets\n",
    "\n",
    "    def calculate_sharpe_ratio(self, ticker):\n",
    "        data = self.env.data[ticker]\n",
    "        returns = data['Close'].pct_change().dropna()\n",
    "        avg_return = returns.mean()\n",
    "        volatility = returns.std()\n",
    "\n",
    "        # Calculate Sharpe ratio (assuming a risk-free rate of 0 for simplicity)\n",
    "        sharpe_ratio = avg_return / volatility\n",
    "        return sharpe_ratio\n",
    "\n",
    "    def calculate_reward(self, current_ticker):\n",
    "        # Calculate portfolio return\n",
    "        self.portfolio_value = self.get_portfolio_value()\n",
    "        portfolio_return = (self.portfolio_value - self.initial_portfolio_value) / self.initial_portfolio_value\n",
    "\n",
    "        # Calculate drawdown\n",
    "        rolling_max = self.env.data[current_ticker]['Close'].cummax()\n",
    "        drawdown = (rolling_max - self.env.data[current_ticker]['Close']) / rolling_max\n",
    "        max_drawdown = drawdown.max()\n",
    "\n",
    "        # Calculate risk-adjusted returns (Sharpe and Sortino ratios)\n",
    "        returns = self.env.data[current_ticker]['Close'].pct_change().dropna()\n",
    "        sharpe_ratio = returns.mean() / returns.std()\n",
    "        sortino_ratio = returns.mean() / returns[returns < 0].std()\n",
    "\n",
    "        # Consistency factor (standard deviation of returns)\n",
    "        consistency = -returns.std()  # Negative because lower volatility (std) is preferred\n",
    "\n",
    "        # Profit target bonus/penalty\n",
    "        profit_target = 0.05  # 5% profit target\n",
    "        profit_bonus = 1 if portfolio_return >= profit_target else -0.5\n",
    "\n",
    "        # Calculate final reward with stronger penalties for drawdowns and rewards for consistency\n",
    "        reward = (portfolio_return \n",
    "                  - 0.7 * max_drawdown \n",
    "                  + 0.4 * sharpe_ratio \n",
    "                  + 0.3 * sortino_ratio \n",
    "                  + 0.3 * consistency \n",
    "                  + profit_bonus)\n",
    "\n",
    "        return reward\n",
    "\n",
    "    \n",
    "    def take_profit(self, current_price, entry_price):\n",
    "        # Implementing a simple take profit mechanism\n",
    "        gain = (current_price - entry_price) / entry_price\n",
    "        if gain >= self.take_profit_threshold:\n",
    "            return True  # Signal to take profit\n",
    "        return False\n",
    "\n",
    "    \n",
    "    def get_portfolio_value(self):\n",
    "        # This method should retrieve the most recent portfolio value from the environment or relevant agent\n",
    "        return self.env.portfolio_value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d996739",
   "metadata": {},
   "source": [
    "## Mid Level Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ece354",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MidLevelAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.strategies = {\n",
    "            0 : self.moving_average_crossover,\n",
    "            1 : self.rsi_reversion,\n",
    "            2 : self.macd_trend_following,\n",
    "            3 : self.bollinger_bands,\n",
    "            4 : self.adx_trend_strength,\n",
    "            5 : self.stochastic_oscillator,\n",
    "            6 : self.volume_price_trend,\n",
    "            7 : self.cci_correction,\n",
    "            8 : self.ema_rsi_combo,\n",
    "            9 : self.ichimoku_cloud,\n",
    "            10 : self.parabolic_sar,\n",
    "            11 : self.momentum,\n",
    "            12 : self.roc_trend,\n",
    "            13 : self.williams_percent_r,\n",
    "            14 : self.keltner_channel,\n",
    "            15 : self.atr_volatility,\n",
    "            16 : self.vwap_mean_reversion,\n",
    "            17 : self.trix_trend_following,\n",
    "            18 : self.donchian_channel,\n",
    "            19 : self.pivot_point_support_resistance\n",
    "        }\n",
    "        \n",
    "        self.strategy_performance = {name: 0 for name in self.strategies}\n",
    "        self.epsilon = 0.5\n",
    "        self.min_epsilon = 0.1\n",
    "        self.decay_rate = 0.995\n",
    "        self.recent_rewards = []\n",
    "        self.state_size = state_size\n",
    "        self.action_size = len(self.strategies)\n",
    "        self.dqn = DQNAgent(state_size, self.action_size)\n",
    "        self.learning_rate = 0.0001\n",
    "        self.max_steps = 1000  # Max steps per episode\n",
    "        self.step_count = 0\n",
    "        \n",
    "    def select_strategy(self, state, epsilon):\n",
    "        action = self.dqn.act(state)  # Select action using DQN\n",
    "        return action\n",
    "    \n",
    "    def take_action(self, action, state):\n",
    "        # Get the strategy based on the action selected by DQN\n",
    "        strategy_function = self.strategies[action]\n",
    "        strategy_signal = strategy_function(state)\n",
    "        reward = self.calculate_reward(strategy_signal, 1)\n",
    "        self.step_count += 1\n",
    "        done = self.step_count >= self.max_steps\n",
    "        return reward, done\n",
    "    \n",
    "    def update_strategy_performance(self, strategy_name, reward, decay=0.99):\n",
    "        # Update the performance score of the strategy based on the received reward\n",
    "        self.strategy_performance[strategy_name] = (\n",
    "            self.strategy_performance[strategy_name] * decay + reward\n",
    "        )\n",
    "\n",
    "    def choose_strategy(self, state):\n",
    "        if random.random() < self.epsilon:\n",
    "            chosen_strategy = random.choice(list(self.strategies.keys()))\n",
    "        else:\n",
    "            state_array = np.array(state).reshape(1, -1)  # Reshape state for DQN\n",
    "            strategy_index = np.argmax(self.dqn_agent.model.predict(state_array))\n",
    "            chosen_strategy = list(self.strategies.keys())[strategy_index]\n",
    "        self.epsilon = max(self.min_epsilon, self.epsilon * self.decay_rate)\n",
    "        return chosen_strategy\n",
    "    \n",
    "    def learn(self, state, action, reward, next_state, done):\n",
    "        state = np.reshape(state, [1, self.state_size])\n",
    "        next_state = np.reshape(next_state, [1, self.state_size])\n",
    "        self.dqn_agent.remember(state, action, reward, next_state, done)\n",
    "        if len(self.dqn_agent.memory) > 32:\n",
    "            self.dqn_agent.replay(32)\n",
    "\n",
    "    def moving_average_crossover(self, state):\n",
    "\n",
    "        short_ma = state[6]\n",
    "        long_ma = state[45]\n",
    "        if short_ma > long_ma:\n",
    "            return 1  # Signal to buy\n",
    "        elif short_ma < long_ma:\n",
    "            return -1  # Signal to sell\n",
    "        return 0  # Signal to hold\n",
    "\n",
    "    def rsi_reversion(self, state):\n",
    "        rsi = state[8]\n",
    "        if rsi < 30:\n",
    "            return 1  # Signal to buy\n",
    "        elif rsi > 70:\n",
    "            return -1  # Signal to sell\n",
    "        return 0  # Signal to hold\n",
    "\n",
    "    def macd_trend_following(self, state):\n",
    "        macd = state[12]\n",
    "        signal = state[13]\n",
    "        if macd > signal:\n",
    "            return 1  # Signal to buy\n",
    "        elif macd < signal:\n",
    "            return -1  # Signal to sell\n",
    "        return 0  # Signal to hold\n",
    "\n",
    "    def bollinger_bands(self, state):\n",
    " \n",
    "        upper_band = state[29]\n",
    "        lower_band = state[30]\n",
    "        close = state[3]\n",
    "        if close < lower_band:\n",
    "            return 1  # Signal to buy\n",
    "        elif close > upper_band:\n",
    "            return -1  # Signal to sell\n",
    "        return 0  # Signal to hold\n",
    "\n",
    "    def adx_trend_strength(self, state):\n",
    "       \n",
    "        adx = state[47]\n",
    "        if adx > 25:\n",
    "            return 1  # Signal to trade based on trend\n",
    "        return 0  # Signal to hold\n",
    "\n",
    "    def stochastic_oscillator(self, state):\n",
    "       \n",
    "\n",
    "        stoch_rsi = state[9]\n",
    "        if stoch_rsi < 0.2:\n",
    "            return 1  # Signal to buy\n",
    "        elif stoch_rsi > 0.8:\n",
    "            return -1  # Signal to sell\n",
    "        return 0  # Signal to hold\n",
    "\n",
    "    def volume_price_trend(self, state):\n",
    "       \n",
    "        vpt = state[43]\n",
    "        if vpt > 0:\n",
    "            return 1  # Signal to buy\n",
    "        elif vpt < 0:\n",
    "            return -1  # Signal to sell\n",
    "        return 0  # Signal to hold\n",
    "\n",
    "    def cci_correction(self, state):\n",
    "       \n",
    "\n",
    "        cci = state[21]\n",
    "        if cci < -100:\n",
    "            return 1  # Signal to buy\n",
    "        elif cci > 100:\n",
    "            return -1  # Signal to sell\n",
    "        return 0  # Signal to hold\n",
    "\n",
    "    def ema_rsi_combo(self, state):\n",
    "       \n",
    "        ema = state[8]\n",
    "        rsi = state[9]\n",
    "        if rsi < 30 and state[3] > ema:\n",
    "            return 1  # Signal to buy\n",
    "        elif rsi > 70 and state[3] < ema:\n",
    "            return -1  # Signal to sell\n",
    "        return 0  # Signal to hold\n",
    "\n",
    "    def ichimoku_cloud(self, state):\n",
    "       \n",
    "        span_a = state[23]\n",
    "        span_b = state[24]\n",
    "        close = state[3]\n",
    "        if close > span_a and close > span_b:\n",
    "            return 1  # Signal to buy\n",
    "        elif close < span_a and close < span_b:\n",
    "            return -1  # Signal to sell\n",
    "        return 0  # Signal to hold\n",
    "\n",
    "    def parabolic_sar(self, state):\n",
    "       \n",
    "\n",
    "        sar = state[16]  # Assume KAMA is used for parabolic SAR substitute\n",
    "        close = state[3]\n",
    "        if close > sar:\n",
    "            return 1  # Signal to buy\n",
    "        elif close < sar:\n",
    "            return -1  # Signal to sell\n",
    "        return 0  # Signal to hold\n",
    "\n",
    "    def momentum(self, state):\n",
    "        \n",
    "\n",
    "        momentum = state[17]\n",
    "        if momentum > 0:\n",
    "            return 1  # Signal to buy\n",
    "        elif momentum < 0:\n",
    "            return -1  # Signal to sell\n",
    "        return 0  # Signal to hold\n",
    "\n",
    "    def roc_trend(self, state):\n",
    "        \n",
    "        roc = state[17]\n",
    "        if roc > 0:\n",
    "            return 1  # Signal to buy\n",
    "        elif roc < 0:\n",
    "            return -1  # Signal to sell\n",
    "        return 0  # Signal to hold\n",
    "\n",
    "    def williams_percent_r(self, state):\n",
    "        \n",
    "        will_r = state[10]\n",
    "        if will_r < -80:\n",
    "            return 1  # Signal to buy\n",
    "        elif will_r > -20:\n",
    "            return -1  # Signal to sell\n",
    "        return 0  # Signal to hold\n",
    "\n",
    "    def keltner_channel(self, state):\n",
    "        \n",
    "\n",
    "        # Example: Assuming `state` has raw data where indices correspond to specific features\n",
    "        close_prices = state[0]  # Example index for 'Close' prices\n",
    "        middle_band = close_prices\n",
    "        upper_band = middle_band + 2 * close_prices\n",
    "        lower_band = middle_band - 2 * close_prices\n",
    "\n",
    "        current_close = close_prices\n",
    "        if current_close > upper_band:\n",
    "            return -1  # Signal to sell\n",
    "        elif current_close < lower_band:\n",
    "            return 1  # Signal to buy\n",
    "        else:\n",
    "            return 0  # Signal to hold\n",
    "        \n",
    "    def atr_volatility(self, state):\n",
    "\n",
    "        atr = state[38]\n",
    "        if atr > atr:\n",
    "            return 1  # Signal to buy (volatility breakout)\n",
    "        return 0  # Signal to hold\n",
    "\n",
    "    def vwap_mean_reversion(self, state):\n",
    "        \n",
    "\n",
    "        vwap = state[44]\n",
    "        close = state[3]\n",
    "        if close < vwap:\n",
    "            return 1  # Signal to buy\n",
    "        elif close > vwap:\n",
    "            return -1  # Signal to sell\n",
    "        return 0  # Signal to hold\n",
    "\n",
    "    def trix_trend_following(self, state):\n",
    "        \n",
    "        trix = state[19]\n",
    "        if trix > 0:\n",
    "            return 1  # Signal to buy\n",
    "        elif trix < 0:\n",
    "            return -1  # Signal to sell\n",
    "        return 0  # Signal to hold\n",
    "\n",
    "    def donchian_channel(self, state):\n",
    "        \n",
    "        upper_band = state[36]\n",
    "        lower_band = state[37]\n",
    "        close = state[3]\n",
    "        if close > upper_band:\n",
    "            return 1  # Signal to buy\n",
    "        elif close < lower_band:\n",
    "            return -1  # Signal to sell\n",
    "        return 0  # Signal to hold\n",
    "\n",
    "    def pivot_point_support_resistance(self, state):\n",
    "        \n",
    "\n",
    "        pivot = state[28]  # Using Bollinger Mid as a pivot substitute\n",
    "        close = state[3]\n",
    "        if close > pivot:\n",
    "            return 1  # Signal to buy\n",
    "        elif close < pivot:\n",
    "            return -1  # Signal to sell\n",
    "        return 0  # Signal to hold\n",
    "\n",
    "    def calculate_reward(self, profit, transaction_costs, duration_penalty=0.01):\n",
    "        net_profit = profit - transaction_costs\n",
    "        time_penalty = duration_penalty\n",
    "        reward = net_profit - time_penalty\n",
    "        return reward\n",
    "    \n",
    "    def reset(self):\n",
    "        self.step_count = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d036996",
   "metadata": {},
   "source": [
    "## Low Level Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d83ebb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LowLevelAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.dqn = DQNAgent(state_size, action_size)\n",
    "        self.max_trades = 10\n",
    "        self.trade_count = 0\n",
    "        self.current_position = None\n",
    "        self.current_price = None\n",
    "\n",
    "    def select_action(self, state):\n",
    "        action = self.dqn.act(state)\n",
    "        return action\n",
    "\n",
    "    def take_action(self, action, state, price):\n",
    "        self.current_price = price\n",
    "\n",
    "        if action == 1:  # Buy\n",
    "            reward = self.buy(state)\n",
    "        elif action == 2:  # Sell\n",
    "            reward = self.sell(state)\n",
    "        else:  # Hold\n",
    "            reward = self.hold(state)\n",
    "        \n",
    "        self.trade_count += 1\n",
    "        done = self.trade_count >= self.max_trades\n",
    "        return reward, done\n",
    "\n",
    "    def buy(self, state):\n",
    "        if self.current_position is not None:\n",
    "            # Cannot buy if already holding a position\n",
    "            return -10  # Penalty for invalid action\n",
    "        \n",
    "        # Simulate buying\n",
    "        self.current_position = 'long'\n",
    "        buy_price = self.current_price\n",
    "        reward = self.calculate_trade_reward(buy_price, 'buy')\n",
    "        return reward\n",
    "\n",
    "    def sell(self, state):\n",
    "        if self.current_position != 'long':\n",
    "            # Cannot sell if not holding a position\n",
    "            return -10  # Penalty for invalid action\n",
    "\n",
    "        # Simulate selling\n",
    "        self.current_position = None\n",
    "        sell_price = self.current_price\n",
    "        reward = self.calculate_trade_reward(sell_price, 'sell')\n",
    "        return reward\n",
    "\n",
    "    def hold(self, state):\n",
    "        # No action taken, so no immediate reward\n",
    "        return 0\n",
    "\n",
    "    def calculate_trade_reward(self, trade_price, action_type):\n",
    "        # Example reward calculation\n",
    "        # Assuming `trade_price` is the price at which the trade was executed\n",
    "        if action_type == 'buy':\n",
    "            # Buying: Reward based on future price changes\n",
    "            future_price = self.get_future_price()  # Placeholder for future price\n",
    "            reward = (future_price - trade_price) * 100  # Example: Profit or Loss\n",
    "        elif action_type == 'sell':\n",
    "            # Selling: Reward based on past price changes\n",
    "            past_price = self.get_past_price()  # Placeholder for past price\n",
    "            reward = (trade_price - past_price) * 100  # Example: Profit or Loss\n",
    "        else:\n",
    "            reward = 0\n",
    "        \n",
    "        return reward\n",
    "\n",
    "    def get_future_price(self):\n",
    "        # Simulate or get future price for reward calculation\n",
    "        return self.current_price * (1 + np.random.uniform(-0.01, 0.01))  # Random price change\n",
    "\n",
    "    def get_past_price(self):\n",
    "        # Simulate or get past price for reward calculation\n",
    "        return self.current_price * (1 + np.random.uniform(-0.01, 0.01))  # Random price change\n",
    "\n",
    "    def reset(self):\n",
    "        self.trade_count = 0\n",
    "        self.current_position = None\n",
    "        self.current_price = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc038d77",
   "metadata": {},
   "source": [
    "## Initializing and Defining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e033990f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mid_level_state(raw_state):\n",
    "    mid_state = {\n",
    "        'Open': raw_state[0],\n",
    "        'High': raw_state[1],\n",
    "        'Low': raw_state[2],\n",
    "        'Close': raw_state[3],\n",
    "        'Adjusted_Close': raw_state[4],\n",
    "        'Volume': raw_state[5],\n",
    "        'Williams_R': raw_state[6],\n",
    "        'Awesome_Oscillator': raw_state[7],\n",
    "        'MACD': raw_state[8],\n",
    "        'MACD_Signal': raw_state[9],\n",
    "        'MACD_Diff': raw_state[10],\n",
    "        'TSI': raw_state[11],\n",
    "        'KAMA': raw_state[12],\n",
    "        'ROC': raw_state[13],\n",
    "        'Vortex_Diff': raw_state[14],\n",
    "        'TRIX': raw_state[15],\n",
    "        'Mass_Index': raw_state[16],\n",
    "        'CCI': raw_state[17],\n",
    "        'DPO': raw_state[18],\n",
    "        'Ichimoku_A': raw_state[19],\n",
    "        'Ichimoku_B': raw_state[20],\n",
    "        'Aroon_Up': raw_state[21],\n",
    "        'Aroon_Down': raw_state[22],\n",
    "        'Aroon_Indicator': raw_state[23],\n",
    "        'Bollinger_Mid': raw_state[24],\n",
    "        'Bollinger_Upper': raw_state[25],\n",
    "        'Bollinger_Lower': raw_state[26],\n",
    "        'Bollinger_PBand': raw_state[27],\n",
    "        'Bollinger_WBand': raw_state[28],\n",
    "        'Keltner_Channel_Center': raw_state[29],\n",
    "        'Keltner_Channel_Upper': raw_state[30],\n",
    "        'Keltner_Channel_Lower': raw_state[31],\n",
    "        'Donchian_Channel_Upper': raw_state[32],\n",
    "        'Donchian_Channel_Lower': raw_state[33],\n",
    "        'ATR': raw_state[34],\n",
    "        'OBV': raw_state[35],\n",
    "        'Chaikin_MF': raw_state[36],\n",
    "        'Force_Index': raw_state[37],\n",
    "        'Ease_of_Movement': raw_state[38],\n",
    "        'Volume_Price_Trend': raw_state[39],\n",
    "        'VWAP': raw_state[40],\n",
    "        'SMA_200': raw_state[41],\n",
    "        'EMA_200': raw_state[42],\n",
    "        'ADX': raw_state[43],\n",
    "        'Vortex_Pos': raw_state[44],\n",
    "        'Vortex_Neg': raw_state[45]\n",
    "    }\n",
    "    return mid_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7c1775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the environment\n",
    "env = TradingEnv(data=processed_data, ticker_list=list(processed_data.keys()))\n",
    "\n",
    "state_size = 66\n",
    "action_size = 20\n",
    "\n",
    "# Initialize the Mid and Low-Level Agents\n",
    "high_agent = HighLevelAgent(env)\n",
    "mid_agent = MidLevelAgent(state_size, action_size)\n",
    "low_agent = LowLevelAgent(state_size, action_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ddedcd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "replay_memory = deque(maxlen=2000)\n",
    "\n",
    "# Initialize optimizers\n",
    "learning_rate = 0.001\n",
    "mid_agent.dqn.model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mse')\n",
    "\n",
    "# For the low-level agent\n",
    "low_agent.dqn.model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5892cdb9",
   "metadata": {},
   "source": [
    "## Training and Model Saving Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25a5b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "epsilon_start = 1.0       # Starting value of epsilon (exploration rate)\n",
    "epsilon_min = 0.01        # Minimum value of epsilon\n",
    "epsilon_decay = 0.995     # Decay rate for epsilon after each episode\n",
    "num_episodes = 1000       # Total number of episodes for training\n",
    "batch_size = 32           # Size of minibatch for DQN updates\n",
    "gamma = 0.99              # Discount factor for future rewards\n",
    "\n",
    "# Learning rates\n",
    "learning_rate = 0.001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afe83f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dqn():\n",
    "    epsilon = epsilon_start\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            agents = high_agent.choose_equities_and_allocate_budget()\n",
    "\n",
    "            for ticker, budget, mid_agent, low_agent in agents:\n",
    "                mid_state = get_mid_level_state(state)\n",
    "                \n",
    "                # Mid-Level Agent: Select strategy using DQN\n",
    "                mid_action = mid_agent.select_strategy(mid_state, epsilon)\n",
    "                mid_reward, done = mid_agent.take_action(mid_action, state)\n",
    "                \n",
    "                # Store transition in replay memory\n",
    "                mid_next_state = env.get_state()\n",
    "                mid_agent.dqn.remember(mid_state, mid_action, mid_reward, mid_next_state, done)\n",
    "                \n",
    "                # Learn from replay memory\n",
    "                if len(mid_agent.dqn.memory) > batch_size:\n",
    "                    mid_agent.dqn.replay(batch_size)\n",
    "\n",
    "                # Low-Level Agent: Implement selected strategy and trade\n",
    "                low_action = low_agent.select_action(mid_state)\n",
    "                low_reward, done = low_agent.take_action(low_action, state, state[0])\n",
    "                \n",
    "                # Store transition in replay memory\n",
    "                low_next_state = env.get_state()\n",
    "                low_agent.dqn.remember(mid_state, low_action, low_reward, low_next_state, done)\n",
    "\n",
    "                # Learn from replay memory\n",
    "                if len(low_agent.dqn.memory) > batch_size:\n",
    "                    low_agent.dqn.replay(batch_size)\n",
    "\n",
    "                total_reward += mid_reward + low_reward\n",
    "\n",
    "            epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "\n",
    "        print(f\"Episode {episode} - Total Reward: {total_reward}\")\n",
    "\n",
    "        if episode % 100 == 0:\n",
    "            torch.save(mid_agent.dqn.model.state_dict(), f'mid_level_dqn_{episode}.pth')\n",
    "            torch.save(low_agent.dqn.model.state_dict(), f'low_level_dqn_{episode}.pth')\n",
    "\n",
    "    torch.save(mid_agent.dqn.model.state_dict(), 'mid_level_dqn_final.pth')\n",
    "    torch.save(low_agent.dqn.model.state_dict(), 'low_level_dqn_final.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246579aa",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_dqn()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
