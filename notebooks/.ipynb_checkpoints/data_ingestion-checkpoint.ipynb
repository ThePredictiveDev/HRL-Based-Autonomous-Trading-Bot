{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921a3e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import os\n",
    "from ta import add_all_ta_features\n",
    "import ta.utils \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import schedule\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import requests\n",
    "from transformers import pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f0f8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_intraday_data(ticker, period=\"5d\", interval=\"1m\"):\n",
    "    \"\"\"Fetches intraday data for the last 5 days with a 1-minute interval.\"\"\"\n",
    "    try:\n",
    "        data = yf.download(ticker, period=period, interval=interval)\n",
    "        if data.empty:\n",
    "            print(f\"No data found for {ticker}\")\n",
    "            return None\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to fetch intraday data for {ticker}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d98a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#List of Indian Midcap Stocks\n",
    "midcap_stocks = [\n",
    "    \"MSUMI.NS\", \"TORNTPOWER.NS\", \"GODREJPROP.NS\", \"SRF.NS\",\n",
    "    \"APLAPOLLO.NS\", \"TVSMOTOR.NS\", \"LTIM.NS\", \"PAGEIND.NS\",\n",
    "    \"AUROPHARMA.NS\", \"JINDALSTEL.NS\", \"BAJAJHLDNG.NS\", \"BATAINDIA.NS\",\n",
    "    \"BHEL.NS\", \"CANBK.NS\", \"CHOLAFIN.NS\", \"CUB.NS\", \"DALMIASUG.NS\",\n",
    "    \"ESCORTS.NS\", \"FEDERALBNK.NS\", \"FORTIS.NS\", \"GICRE.NS\",\n",
    "    \"GMRINFRA.NS\", \"GNFC.NS\", \"GODREJAGRO.NS\", \"GRASIM.NS\", \"HAVELLS.NS\",\n",
    "    \"HINDPETRO.NS\", \"INDHOTEL.NS\", \"JUBLFOOD.NS\", \"LICHSGFIN.NS\",\n",
    "    \"M&MFIN.NS\", \"MANAPPURAM.NS\", \"MRF.NS\", \"NATCOPHARM.NS\",\n",
    "    \"NCC.NS\", \"NMDC.NS\", \"OBEROIRLTY.NS\", \"PERSISTENT.NS\", \"PETRONET.NS\",\n",
    "    \"RAMCOCEM.NS\", \"RBLBANK.NS\", \"SAIL.NS\", \"SUNTV.NS\", \"TATACOMM.NS\",\n",
    "    \"TATAPOWER.NS\", \"THYROCARE.NS\", \"TORNTPHARM.NS\", \"TRENT.NS\", \"VOLTAS.NS\",\n",
    "    \"WHIRLPOOL.NS\", \"YESBANK.NS\", \"ZEEL.NS\", \"ZYDUSWELL.NS\",\n",
    "    \"ABBOTINDIA.NS\", \"ASHOKLEY.NS\", \"BALKRISIND.NS\", \"BEL.NS\", \"CONCOR.NS\",\n",
    "    \"CROMPTON.NS\", \"DEEPAKNTR.NS\", \"DIXON.NS\", \"EMAMILTD.NS\",\n",
    "    \"INDIAMART.NS\", \"IRCTC.NS\", \"JUBLPHARMA.NS\", \"LTTS.NS\", \"MFSL.NS\",\n",
    "    \"METROPOLIS.NS\", \"OBEROIRLTY.NS\", \"PIIND.NS\", \"POLYCAB.NS\", \"RECLTD.NS\",\n",
    "    \"SUPREMEIND.NS\", \"TATACONSUM.NS\", \"TV18BRDCST.NS\", \"VGUARD.NS\",\n",
    "    \"VBL.NS\", \"VINATIORGA.NS\", \"ZENSARTECH.NS\", \"IDFCFIRSTB.NS\",\n",
    "    \"SONACOMS.NS\", \"AMBUJACEM.NS\", \"GAIL.NS\", \"TATAELXSI.NS\", \"MAXHEALTH.NS\",\n",
    "    \"LALPATHLAB.NS\", \"JSWENERGY.NS\", \"AARTIIND.NS\", \"ADANIGREEN.NS\",\n",
    "    \"ABFRL.NS\", \"BANDHANBNK.NS\", \"BANKINDIA.NS\", \"BERGEPAINT.NS\", \"BOSCHLTD.NS\",\n",
    "    \"CUMMINSIND.NS\", \"DMART.NS\", \"GLENMARK.NS\", \"GUJGASLTD.NS\",\n",
    "    \"HAL.NS\", \"IIFLWAM.NS\", \"LICI.NS\", \"LUXIND.NS\", \"M&MFIN.NS\",\n",
    "    \"NAUKRI.NS\", \"PHOENIXLTD.NS\", \"RAJESHEXPO.NS\", \"SHREECEM.NS\",\n",
    "    \"TATACHEM.NS\", \"THERMAX.NS\", \"TTKPRESTIG.NS\", \"UJJIVANSFB.NS\", \"VAKRANGEE.NS\"\n",
    "]\n",
    "\n",
    "ticker_to_company_name = {\n",
    "    \"MSUMI.NS\": \"Motherson Sumi Systems Ltd.\",\n",
    "    \"TORNTPOWER.NS\": \"Torrent Power Ltd.\",\n",
    "    \"GODREJPROP.NS\": \"Godrej Properties Ltd.\",\n",
    "    \"SRF.NS\": \"SRF Ltd.\",\n",
    "    \"APLAPOLLO.NS\": \"APL Apollo Tubes Ltd.\",\n",
    "    \"TVSMOTOR.NS\": \"TVS Motor Company Ltd.\",\n",
    "    \"PAGEIND.NS\": \"Page Industries Ltd.\",\n",
    "    \"AUROPHARMA.NS\": \"Aurobindo Pharma Ltd.\",\n",
    "    \"JINDALSTEL.NS\": \"Jindal Steel & Power Ltd.\",\n",
    "    \"BAJAJHLDNG.NS\": \"Bajaj Holdings & Investment Ltd.\",\n",
    "    \"BATAINDIA.NS\": \"Bata India Ltd.\",\n",
    "    \"BHEL.NS\": \"Bharat Heavy Electricals Ltd.\",\n",
    "    \"CANBK.NS\": \"Canara Bank\",\n",
    "    \"CHOLAFIN.NS\": \"Cholamandalam Investment and Finance Company Ltd.\",\n",
    "    \"CUB.NS\": \"City Union Bank Ltd.\",\n",
    "    \"DALMIASUG.NS\": \"Dalmia Bharat Sugar and Industries Ltd.\",\n",
    "    \"ESCORTS.NS\": \"Escorts Ltd.\",\n",
    "    \"FEDERALBNK.NS\": \"The Federal Bank Ltd.\",\n",
    "    \"FORTIS.NS\": \"Fortis Healthcare Ltd.\",\n",
    "    \"GICRE.NS\": \"General Insurance Corporation of India\",\n",
    "    \"GMRINFRA.NS\": \"GMR Infrastructure Ltd.\",\n",
    "    \"GNFC.NS\": \"Gujarat Narmada Valley Fertilizers & Chemicals Ltd.\",\n",
    "    \"GODREJAGRO.NS\": \"Godrej Agrovet Ltd.\",\n",
    "    \"GRASIM.NS\": \"Grasim Industries Ltd.\",\n",
    "    \"HAVELLS.NS\": \"Havells India Ltd.\",\n",
    "    \"HINDPETRO.NS\": \"Hindustan Petroleum Corporation Ltd.\",\n",
    "    \"INDHOTEL.NS\": \"The Indian Hotels Company Ltd.\",\n",
    "    \"JUBLFOOD.NS\": \"Jubilant FoodWorks Ltd.\",\n",
    "    \"LICHSGFIN.NS\": \"LIC Housing Finance Ltd.\",\n",
    "    \"M&MFIN.NS\": \"Mahindra & Mahindra Financial Services Ltd.\",\n",
    "    \"MANAPPURAM.NS\": \"Manappuram Finance Ltd.\",\n",
    "    \"MRF.NS\": \"MRF Ltd.\",\n",
    "    \"NATCOPHARM.NS\": \"Natco Pharma Ltd.\",\n",
    "    \"NCC.NS\": \"NCC Ltd.\",\n",
    "    \"NMDC.NS\": \"NMDC Ltd.\",\n",
    "    \"OBEROIRLTY.NS\": \"Oberoi Realty Ltd.\",\n",
    "    \"PERSISTENT.NS\": \"Persistent Systems Ltd.\",\n",
    "    \"PETRONET.NS\": \"Petronet LNG Ltd.\",\n",
    "    \"RAMCOCEM.NS\": \"The Ramco Cements Ltd.\",\n",
    "    \"RBLBANK.NS\": \"RBL Bank Ltd.\",\n",
    "    \"SAIL.NS\": \"Steel Authority of India Ltd.\",\n",
    "    \"SUNTV.NS\": \"Sun TV Network Ltd.\",\n",
    "    \"TATACOMM.NS\": \"Tata Communications Ltd.\",\n",
    "    \"TATAPOWER.NS\": \"Tata Power Company Ltd.\",\n",
    "    \"THYROCARE.NS\": \"Thyrocare Technologies Ltd.\",\n",
    "    \"TORNTPHARM.NS\": \"Torrent Pharmaceuticals Ltd.\",\n",
    "    \"TRENT.NS\": \"Trent Ltd.\",\n",
    "    \"VOLTAS.NS\": \"Voltas Ltd.\",\n",
    "    \"WHIRLPOOL.NS\": \"Whirlpool of India Ltd.\",\n",
    "    \"YESBANK.NS\": \"Yes Bank Ltd.\",\n",
    "    \"ZEEL.NS\": \"Zee Entertainment Enterprises Ltd.\",\n",
    "    \"ZYDUSWELL.NS\": \"Zydus Wellness Ltd.\",\n",
    "    \"ABBOTINDIA.NS\": \"Abbott India Ltd.\",\n",
    "    \"ASHOKLEY.NS\": \"Ashok Leyland Ltd.\",\n",
    "    \"BALKRISIND.NS\": \"Balkrishna Industries Ltd.\",\n",
    "    \"BEL.NS\": \"Bharat Electronics Ltd.\",\n",
    "    \"CONCOR.NS\": \"Container Corporation of India Ltd.\",\n",
    "    \"CROMPTON.NS\": \"Crompton Greaves Consumer Electricals Ltd.\",\n",
    "    \"DEEPAKNTR.NS\": \"Deepak Nitrite Ltd.\",\n",
    "    \"DIXON.NS\": \"Dixon Technologies (India) Ltd.\",\n",
    "    \"EMAMILTD.NS\": \"Emami Ltd.\",\n",
    "    \"INDIAMART.NS\": \"IndiaMART InterMESH Ltd.\",\n",
    "    \"IRCTC.NS\": \"Indian Railway Catering and Tourism Corporation Ltd.\",\n",
    "    \"JUBLPHARMA.NS\": \"Jubilant Pharmova Ltd.\",\n",
    "    \"LTTS.NS\": \"L&T Technology Services Ltd.\",\n",
    "    \"MFSL.NS\": \"Max Financial Services Ltd.\",\n",
    "    \"METROPOLIS.NS\": \"Metropolis Healthcare Ltd.\",\n",
    "    \"OBEROIRLTY.NS\": \"Oberoi Realty Ltd.\",\n",
    "    \"PIIND.NS\": \"PI Industries Ltd.\",\n",
    "    \"POLYCAB.NS\": \"Polycab India Ltd.\",\n",
    "    \"RECLTD.NS\": \"REC Ltd.\",\n",
    "    \"SUPREMEIND.NS\": \"Supreme Industries Ltd.\",\n",
    "    \"TATACONSUM.NS\": \"Tata Consumer Products Ltd.\",\n",
    "    \"TV18BRDCST.NS\": \"TV18 Broadcast Ltd.\",\n",
    "    \"VGUARD.NS\": \"V-Guard Industries Ltd.\",\n",
    "    \"VBL.NS\": \"Varun Beverages Ltd.\",\n",
    "    \"VINATIORGA.NS\": \"Vinati Organics Ltd.\",\n",
    "    \"ZENSARTECH.NS\": \"Zensar Technologies Ltd.\",\n",
    "    \"IDFCFIRSTB.NS\": \"IDFC First Bank Ltd.\",\n",
    "    \"SONACOMS.NS\": \"Sona BLW Precision Forgings Ltd.\",\n",
    "    \"AMBUJACEM.NS\": \"Ambuja Cements Ltd.\",\n",
    "    \"GAIL.NS\": \"GAIL (India) Ltd.\",\n",
    "    \"TATAELXSI.NS\": \"Tata Elxsi Ltd.\",\n",
    "    \"MAXHEALTH.NS\": \"Max Healthcare Institute Ltd.\",\n",
    "    \"LALPATHLAB.NS\": \"Dr. Lal PathLabs Ltd.\",\n",
    "    \"JSWENERGY.NS\": \"JSW Energy Ltd.\",\n",
    "    \"AARTIIND.NS\": \"Aarti Industries Ltd.\",\n",
    "    \"ADANIGREEN.NS\": \"Adani Green Energy Ltd.\",\n",
    "    \"ABFRL.NS\": \"Aditya Birla Fashion and Retail Ltd.\",\n",
    "    \"BANDHANBNK.NS\": \"Bandhan Bank Ltd.\",\n",
    "    \"BANKINDIA.NS\": \"Bank of India\",\n",
    "    \"BERGEPAINT.NS\": \"Berger Paints India Ltd.\",\n",
    "    \"BOSCHLTD.NS\": \"Bosch Ltd.\",\n",
    "    \"CUMMINSIND.NS\": \"Cummins India Ltd.\",\n",
    "    \"DMART.NS\": \"Avenue Supermarts Ltd.\",\n",
    "    \"GLENMARK.NS\": \"Glenmark Pharmaceuticals Ltd.\",\n",
    "    \"GUJGASLTD.NS\": \"Gujarat Gas Ltd.\",\n",
    "    \"HAL.NS\": \"Hindustan Aeronautics Ltd.\",\n",
    "    \"LICI.NS\": \"Life Insurance Corporation of India\",\n",
    "    \"LUXIND.NS\": \"Lux Industries Ltd.\",\n",
    "    \"NAUKRI.NS\": \"Info Edge (India) Ltd.\",\n",
    "    \"PHOENIXLTD.NS\": \"The Phoenix Mills Ltd.\",\n",
    "    \"RAJESHEXPO.NS\": \"Rajesh Exports Ltd.\",\n",
    "    \"SHREECEM.NS\": \"Shree Cement Ltd.\",\n",
    "    \"TATACHEM.NS\": \"Tata Chemicals Ltd.\",\n",
    "    \"THERMAX.NS\": \"Thermax Ltd.\",\n",
    "    \"TTKPRESTIG.NS\": \"TTK Prestige Ltd.\",\n",
    "    \"UJJIVANSFB.NS\": \"Ujjivan Small Finance Bank Ltd.\",\n",
    "    \"VAKRANGEE.NS\": \"Vakrangee Ltd.\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2928d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#directory to save the processed data\n",
    "os.makedirs('data/processed', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95493e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    df = df.fillna(method='ffill')\n",
    "    df = df.fillna(method='bfill')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9886e3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_technical_indicators(df):\n",
    "    # Ensure no missing values (na) are present in the data\n",
    "    \n",
    "    if len(df) < 100:  #threshold, adjust as needed\n",
    "        print(f\"Insufficient data to calculate all indicators for this stock. Data length: {len(df)}\")\n",
    "        return df\n",
    "    \n",
    "    df = ta.utils.dropna(df)\n",
    "\n",
    "    # Momentum Indicators\n",
    "    df['SMA_50'] = ta.trend.SMAIndicator(close=df['Close'], window=50).sma_indicator()\n",
    "    df['EMA_50'] = ta.trend.EMAIndicator(close=df['Close'], window=50).ema_indicator()\n",
    "    df['RSI'] = ta.momentum.RSIIndicator(close=df['Close']).rsi()\n",
    "    df['Stoch_RSI'] = ta.momentum.StochRSIIndicator(close=df['Close']).stochrsi()\n",
    "    df['Williams_R'] = ta.momentum.WilliamsRIndicator(high=df['High'], low=df['Low'], close=df['Close']).williams_r()\n",
    "    df['Awesome_Oscillator'] = ta.momentum.AwesomeOscillatorIndicator(high=df['High'], low=df['Low']).awesome_oscillator()\n",
    "    df['MACD'] = ta.trend.MACD(close=df['Close']).macd()\n",
    "    df['MACD_Signal'] = ta.trend.MACD(close=df['Close']).macd_signal()\n",
    "    df['MACD_Diff'] = ta.trend.MACD(close=df['Close']).macd_diff()\n",
    "    df['TSI'] = ta.momentum.TSIIndicator(close=df['Close']).tsi()\n",
    "    df['KAMA'] = ta.momentum.KAMAIndicator(close=df['Close']).kama()\n",
    "    df['ROC'] = ta.momentum.ROCIndicator(close=df['Close']).roc()\n",
    "\n",
    "    # Trend Indicators\n",
    "    df['Vortex_Diff'] = ta.trend.VortexIndicator(high=df['High'], low=df['Low'], close=df['Close']).vortex_indicator_diff()\n",
    "    df['TRIX'] = ta.trend.TRIXIndicator(close=df['Close']).trix()\n",
    "    df['Mass_Index'] = ta.trend.MassIndex(high=df['High'], low=df['Low']).mass_index()\n",
    "    df['CCI'] = ta.trend.CCIIndicator(high=df['High'], low=df['Low'], close=df['Close']).cci()\n",
    "    df['DPO'] = ta.trend.DPOIndicator(close=df['Close']).dpo()\n",
    "    df['Ichimoku_A'] = ta.trend.IchimokuIndicator(high=df['High'], low=df['Low']).ichimoku_a()\n",
    "    df['Ichimoku_B'] = ta.trend.IchimokuIndicator(high=df['High'], low=df['Low']).ichimoku_b()\n",
    "    #Aroon Calculation\n",
    "    window = 25\n",
    "    rolling_high = df['Close'].rolling(window=window, min_periods=1).max()\n",
    "    rolling_low = df['Close'].rolling(window=window, min_periods=1).min()\n",
    "    df['Aroon_Up'] = 100 * df['Close'].rolling(window=window).apply(lambda x: (x.argmax() + 1) / window, raw=True)\n",
    "    df['Aroon_Down'] = 100 * df['Close'].rolling(window=window).apply(lambda x: (x.argmin() + 1) / window, raw=True)\n",
    "    df['Aroon_Indicator'] = df['Aroon_Up'] - df['Aroon_Down']\n",
    "        \n",
    "    # Volatility Indicators\n",
    "    df['Bollinger_Mid'] = ta.volatility.BollingerBands(close=df['Close']).bollinger_mavg()\n",
    "    df['Bollinger_Upper'] = ta.volatility.BollingerBands(close=df['Close']).bollinger_hband()\n",
    "    df['Bollinger_Lower'] = ta.volatility.BollingerBands(close=df['Close']).bollinger_lband()\n",
    "    df['Bollinger_PBand'] = ta.volatility.BollingerBands(close=df['Close']).bollinger_pband()\n",
    "    df['Bollinger_WBand'] = ta.volatility.BollingerBands(close=df['Close']).bollinger_wband()\n",
    "    df['Keltner_Channel_Center'] = ta.volatility.KeltnerChannel(high=df['High'], low=df['Low'], close=df['Close']).keltner_channel_mband()\n",
    "    df['Keltner_Channel_Upper'] = ta.volatility.KeltnerChannel(high=df['High'], low=df['Low'], close=df['Close']).keltner_channel_hband()\n",
    "    df['Keltner_Channel_Lower'] = ta.volatility.KeltnerChannel(high=df['High'], low=df['Low'], close=df['Close']).keltner_channel_lband()\n",
    "    df['Donchian_Channel_Upper'] = ta.volatility.DonchianChannel(high=df['High'], low=df['Low'], close=df['Close']).donchian_channel_hband()\n",
    "    df['Donchian_Channel_Lower'] = ta.volatility.DonchianChannel(high=df['High'], low=df['Low'], close=df['Close']).donchian_channel_lband()\n",
    "    df['ATR'] = ta.volatility.AverageTrueRange(high=df['High'], low=df['Low'], close=df['Close']).average_true_range()\n",
    "\n",
    "    # Volume Indicators\n",
    "    df['OBV'] = ta.volume.OnBalanceVolumeIndicator(close=df['Close'], volume=df['Volume']).on_balance_volume()\n",
    "    df['Chaikin_MF'] = ta.volume.ChaikinMoneyFlowIndicator(high=df['High'], low=df['Low'], close=df['Close'], volume=df['Volume']).chaikin_money_flow()\n",
    "    df['Force_Index'] = ta.volume.ForceIndexIndicator(close=df['Close'], volume=df['Volume']).force_index()\n",
    "    df['Ease_of_Movement'] = ta.volume.EaseOfMovementIndicator(high=df['High'], low=df['Low'], volume=df['Volume']).ease_of_movement()\n",
    "    df['Volume_Price_Trend'] = ta.volume.VolumePriceTrendIndicator(close=df['Close'], volume=df['Volume']).volume_price_trend()\n",
    "    df['VWAP'] = ta.volume.VolumeWeightedAveragePrice(high=df['High'], low=df['Low'], close=df['Close'], volume=df['Volume']).volume_weighted_average_price()\n",
    "    \n",
    "    try:\n",
    "        df['SMA_200'] = ta.trend.SMAIndicator(close=df['Close'], window=200).sma_indicator()\n",
    "        df['EMA_200'] = ta.trend.EMAIndicator(close=df['Close'], window=200).ema_indicator()\n",
    "        df['ADX'] = ta.trend.ADXIndicator(high=df['High'], low=df['Low'], close=df['Close']).adx()\n",
    "        df['Vortex_Pos'] = ta.trend.VortexIndicator(high=df['High'], low=df['Low'], close=df['Close']).vortex_indicator_pos()\n",
    "        df['Vortex_Neg'] = ta.trend.VortexIndicator(high=df['High'], low=df['Low'], close=df['Close']).vortex_indicator_neg()\n",
    "    except ValueError as e:\n",
    "        print(f\"Could not calculate some indicators due to insufficient data: {e}\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cdcc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_pca(df, n_components=0.95):\n",
    "    # Debugging: Print the columns before PCA\n",
    "    print(f\"Columns before PCA: {df.columns.tolist()}\")\n",
    "    \n",
    "    # Exclude critical columns like 'Close' from PCA\n",
    "    critical_columns = ['Close']\n",
    "    non_critical_features = df.drop(columns=critical_columns, errors='ignore')\n",
    "\n",
    "    # Handle missing values by imputing with column means\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    features_imputed = imputer.fit_transform(non_critical_features)\n",
    "\n",
    "    pca = PCA(n_components=n_components)\n",
    "    pca_features = pca.fit_transform(features_imputed)\n",
    "    \n",
    "    # Convert back to DataFrame\n",
    "    pca_df = pd.DataFrame(pca_features, columns=[f'PC{i+1}' for i in range(pca_features.shape[1])])\n",
    "    \n",
    "    # Reattach the critical columns\n",
    "    pca_df = pd.concat([df[critical_columns].reset_index(drop=True), pca_df], axis=1)\n",
    "    \n",
    "    return pca_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a3b393",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lagged_features(df, columns, lags=[1, 3, 5, 10]):\n",
    "    for column in columns:\n",
    "        for lag in lags:\n",
    "            df[f'{column}_lag_{lag}'] = df[column].shift(lag)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01712461",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEWS_API_KEY =  'c3cee8f6f03c4788b3b68bc89cdbae42'\n",
    "sentiment_analyzer = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68b2815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOT WORKING\n",
    "\n",
    "def fetch_news_sentiment(ticker, company_name, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Fetches and analyzes news sentiment for a given company between the start and end dates using BERT.\n",
    "    \"\"\"\n",
    "    url = f\"https://newsapi.org/v2/top-headlines?country=in&category=business&apiKey={NEWS_API_KEY}\"\n",
    "    \n",
    "    # Retry strategy\n",
    "    retry_strategy = Retry(\n",
    "        total=5,  # Number of retries\n",
    "        backoff_factor=1,  # Wait 1 second between retries, then 2s, 4s, etc.\n",
    "        status_forcelist=[429, 500, 502, 503, 504],\n",
    "        method_whitelist=[\"HEAD\", \"GET\", \"OPTIONS\"]\n",
    "    )\n",
    "    \n",
    "    adapter = HTTPAdapter(max_retries=retry_strategy)\n",
    "    http = requests.Session()\n",
    "    http.mount(\"https://\", adapter)\n",
    "    \n",
    "    try:\n",
    "        response = http.get(url, verify=False)  # Disabling SSL verification (temporary solution)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to fetch news for {company_name}. Status Code: {response.status_code}\")\n",
    "            return pd.DataFrame()  # Return an empty DataFrame in case of failure\n",
    "        \n",
    "        articles = response.json().get(\"articles\", [])\n",
    "        \n",
    "        if not articles:\n",
    "            print(f\"No articles found for {company_name}.\")\n",
    "            return pd.DataFrame()  # Return an empty DataFrame if no articles are found\n",
    "        \n",
    "        # Analyze sentiment of the news articles using BERT\n",
    "        sentiment_data = []\n",
    "        for article in articles:\n",
    "            title = article[\"title\"]\n",
    "            date = article[\"publishedAt\"][:10]  # Extract date from timestamp\n",
    "            sentiment = sentiment_analyzer(title)[0]\n",
    "            sentiment_score = sentiment[\"score\"] if sentiment[\"label\"] == \"POSITIVE\" else -sentiment[\"score\"]\n",
    "            sentiment_data.append({\"Date\": date, \"Sentiment\": sentiment_score})\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        sentiment_df = pd.DataFrame(sentiment_data)\n",
    "        \n",
    "        # Aggregate by date\n",
    "        daily_sentiment = sentiment_df.groupby(\"Date\").mean().reset_index()\n",
    "        daily_sentiment[\"Ticker\"] = ticker\n",
    "        \n",
    "        return daily_sentiment\n",
    "\n",
    "    except requests.exceptions.SSLError as e:\n",
    "        print(f\"SSL Error occurred: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Request failed: {e}\")\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe8fb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sentiment(news_df):\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    news_df['Sentiment'] = news_df['Headline'].apply(lambda x: analyzer.polarity_scores(x)['compound'])\n",
    "    return news_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152cbf61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_data():\n",
    "    for ticker in midcap_stocks:\n",
    "        company_name = ' '.join(ticker.split('.')[0].split('_'))  # Example conversion from ticker to name\n",
    "        process_stock_data(ticker, company_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ffea3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_stock_data(ticker, company_name):\n",
    "    print(f\"Processing data for {ticker}...\")\n",
    "\n",
    "    # Attempt to download historical data\n",
    "    try:\n",
    "        data = yf.download(ticker, start=\"2014-01-01\", end=\"2024-01-01\", interval = '1m')\n",
    "        if data.empty:\n",
    "            print(f\"Data download failed for {ticker}. Moving to the next stock.\")\n",
    "            return  # Skip processing this stock\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download data for {ticker}: {e}\")\n",
    "        return  # Skip processing this stock\n",
    "\n",
    "    # Proceed with data processing if download was successful\n",
    "    data = clean_data(data)\n",
    "    data = add_technical_indicators(data)\n",
    "\n",
    "    # Create lagged features\n",
    "    important_indicators = ['Close', 'RSI', 'MACD', 'Bollinger_Mid']\n",
    "    data = create_lagged_features(data, important_indicators)\n",
    "    data.dropna(inplace=True)  # Drop rows with NaN values due to lagging\n",
    "\n",
    "    # Save the processed data\n",
    "    data.to_csv(f'data/processed/{ticker}_final.csv', index=False)\n",
    "\n",
    "    print(f\"Finished processing data for {ticker}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcc6794",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_save_intraday_data(ticker, output_dir):\n",
    "    \"\"\"Fetches, processes, and saves intraday data for a given ticker.\"\"\"\n",
    "    intraday_data = fetch_intraday_data(ticker)\n",
    "    if intraday_data is not None:\n",
    "        processed_data = add_technical_indicators(intraday_data)\n",
    "        processed_data.dropna(inplace=True)\n",
    "        \n",
    "        processed_data.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "        # Create the output directory if it doesn't exist\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "        \n",
    "        save_path = os.path.join(output_dir, f\"{ticker}_intraday_processed.csv\")\n",
    "        processed_data.to_csv(save_path, index=True)\n",
    "        print(f\"Processed intraday data for {ticker} saved to {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6722ccef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_intraday_data(tickers, output_dir=\"data/processed_intra_day\"):\n",
    "    \"\"\"Processes intraday data for all tickers.\"\"\"\n",
    "    for ticker in tickers:\n",
    "        process_and_save_intraday_data(ticker, output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a7af6b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for ticker, company_name in ticker_to_company_name.items():\n",
    "    process_stock_data(ticker, company_name)\n",
    "\n",
    "print(\"Data processing complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f44125",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_all_intraday_data(midcap_stocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6fb30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "schedule.every().day.at(\"18:00\").do(update_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfebeaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    schedule.run_pending()\n",
    "    time.sleep(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
