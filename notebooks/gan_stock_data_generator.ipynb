{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d90e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import ta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed83544",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd09e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, noise_dim, output_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(noise_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, output_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Define the Discriminator model\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28657fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gan_for_stock(stock_data, num_epochs, noise_dim, lr_g=1e-4, lr_d=1e-4):\n",
    "    # Ensure stock_data is a DataFrame with a 'Close' column\n",
    "    if isinstance(stock_data, pd.DataFrame) and 'Close' in stock_data.columns:\n",
    "        train_data = stock_data['Close'].values.reshape(-1, 1)  # Only train on Close prices\n",
    "    else:\n",
    "        raise ValueError(\"stock_data must be a DataFrame containing a 'Close' column\")\n",
    "\n",
    "    num_features = train_data.shape[1]\n",
    "\n",
    "    # Initialize models\n",
    "    generator = Generator(noise_dim, num_features).to(device)\n",
    "    discriminator = Discriminator(num_features).to(device)\n",
    "\n",
    "    # Optimizers\n",
    "    optimizer_g = optim.Adam(generator.parameters(), lr=lr_g)\n",
    "    optimizer_d = optim.Adam(discriminator.parameters(), lr=lr_d)\n",
    "\n",
    "    # Loss function\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # Generate fake data\n",
    "        noise = torch.randn(train_data.shape[0], noise_dim).to(device)\n",
    "        fake_data = generator(noise)\n",
    "\n",
    "        # Real data\n",
    "        real_data = torch.tensor(train_data, dtype=torch.float).to(device)\n",
    "\n",
    "        # Train Discriminator\n",
    "        optimizer_d.zero_grad()\n",
    "        real_loss = criterion(discriminator(real_data), torch.ones(real_data.size(0), 1).to(device))\n",
    "        fake_loss = criterion(discriminator(fake_data.detach()), torch.zeros(fake_data.size(0), 1).to(device))\n",
    "        d_loss = real_loss + fake_loss\n",
    "        d_loss.backward()\n",
    "        optimizer_d.step()\n",
    "\n",
    "        # Train Generator\n",
    "        optimizer_g.zero_grad()\n",
    "        g_loss = criterion(discriminator(fake_data), torch.ones(fake_data.size(0), 1).to(device))\n",
    "        g_loss.backward()\n",
    "        optimizer_g.step()\n",
    "\n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], d_loss: {d_loss.item()}, g_loss: {g_loss.item()}')\n",
    "\n",
    "    return generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171531d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recalculate_indicators(synthetic_data):\n",
    "    df = pd.DataFrame(synthetic_data, columns=['Open', 'High', 'Low', 'Close', 'Volume'])\n",
    "    \n",
    "    df['SMA_50'] = ta.trend.SMAIndicator(close=df['Close'], window=50).sma_indicator()\n",
    "    df['EMA_50'] = ta.trend.EMAIndicator(close=df['Close'], window=50).ema_indicator()\n",
    "    df['RSI'] = ta.momentum.RSIIndicator(close=df['Close']).rsi()\n",
    "    df['Stoch_RSI'] = ta.momentum.StochRSIIndicator(close=df['Close']).stochrsi()\n",
    "    df['Williams_R'] = ta.momentum.WilliamsRIndicator(high=df['High'], low=df['Low'], close=df['Close']).williams_r()\n",
    "    df['Awesome_Oscillator'] = ta.momentum.AwesomeOscillatorIndicator(high=df['High'], low=df['Low']).awesome_oscillator()\n",
    "    df['MACD'] = ta.trend.MACD(close=df['Close']).macd()\n",
    "    df['MACD_Signal'] = ta.trend.MACD(close=df['Close']).macd_signal()\n",
    "    df['MACD_Diff'] = ta.trend.MACD(close=df['Close']).macd_diff()\n",
    "    df['TSI'] = ta.momentum.TSIIndicator(close=df['Close']).tsi()\n",
    "    df['KAMA'] = ta.momentum.KAMAIndicator(close=df['Close']).kama()\n",
    "    df['ROC'] = ta.momentum.ROCIndicator(close=df['Close']).roc()\n",
    "\n",
    "    # Trend Indicators\n",
    "    df['Vortex_Diff'] = ta.trend.VortexIndicator(high=df['High'], low=df['Low'], close=df['Close']).vortex_indicator_diff()\n",
    "    df['TRIX'] = ta.trend.TRIXIndicator(close=df['Close']).trix()\n",
    "    df['Mass_Index'] = ta.trend.MassIndex(high=df['High'], low=df['Low']).mass_index()\n",
    "    df['CCI'] = ta.trend.CCIIndicator(high=df['High'], low=df['Low'], close=df['Close']).cci()\n",
    "    df['DPO'] = ta.trend.DPOIndicator(close=df['Close']).dpo()\n",
    "    df['Ichimoku_A'] = ta.trend.IchimokuIndicator(high=df['High'], low=df['Low']).ichimoku_a()\n",
    "    df['Ichimoku_B'] = ta.trend.IchimokuIndicator(high=df['High'], low=df['Low']).ichimoku_b()\n",
    "    #Aroon Calculation\n",
    "    window = 25\n",
    "    rolling_high = df['Close'].rolling(window=window, min_periods=1).max()\n",
    "    rolling_low = df['Close'].rolling(window=window, min_periods=1).min()\n",
    "    df['Aroon_Up'] = 100 * df['Close'].rolling(window=window).apply(lambda x: (x.argmax() + 1) / window, raw=True)\n",
    "    df['Aroon_Down'] = 100 * df['Close'].rolling(window=window).apply(lambda x: (x.argmin() + 1) / window, raw=True)\n",
    "    df['Aroon_Indicator'] = df['Aroon_Up'] - df['Aroon_Down']\n",
    "        \n",
    "    # Volatility Indicators\n",
    "    df['Bollinger_Mid'] = ta.volatility.BollingerBands(close=df['Close']).bollinger_mavg()\n",
    "    df['Bollinger_Upper'] = ta.volatility.BollingerBands(close=df['Close']).bollinger_hband()\n",
    "    df['Bollinger_Lower'] = ta.volatility.BollingerBands(close=df['Close']).bollinger_lband()\n",
    "    df['Bollinger_PBand'] = ta.volatility.BollingerBands(close=df['Close']).bollinger_pband()\n",
    "    df['Bollinger_WBand'] = ta.volatility.BollingerBands(close=df['Close']).bollinger_wband()\n",
    "    df['Keltner_Channel_Center'] = ta.volatility.KeltnerChannel(high=df['High'], low=df['Low'], close=df['Close']).keltner_channel_mband()\n",
    "    df['Keltner_Channel_Upper'] = ta.volatility.KeltnerChannel(high=df['High'], low=df['Low'], close=df['Close']).keltner_channel_hband()\n",
    "    df['Keltner_Channel_Lower'] = ta.volatility.KeltnerChannel(high=df['High'], low=df['Low'], close=df['Close']).keltner_channel_lband()\n",
    "    df['Donchian_Channel_Upper'] = ta.volatility.DonchianChannel(high=df['High'], low=df['Low'], close=df['Close']).donchian_channel_hband()\n",
    "    df['Donchian_Channel_Lower'] = ta.volatility.DonchianChannel(high=df['High'], low=df['Low'], close=df['Close']).donchian_channel_lband()\n",
    "    df['ATR'] = ta.volatility.AverageTrueRange(high=df['High'], low=df['Low'], close=df['Close']).average_true_range()\n",
    "\n",
    "    # Volume Indicators\n",
    "    df['OBV'] = ta.volume.OnBalanceVolumeIndicator(close=df['Close'], volume=df['Volume']).on_balance_volume()\n",
    "    df['Chaikin_MF'] = ta.volume.ChaikinMoneyFlowIndicator(high=df['High'], low=df['Low'], close=df['Close'], volume=df['Volume']).chaikin_money_flow()\n",
    "    df['Force_Index'] = ta.volume.ForceIndexIndicator(close=df['Close'], volume=df['Volume']).force_index()\n",
    "    df['Ease_of_Movement'] = ta.volume.EaseOfMovementIndicator(high=df['High'], low=df['Low'], volume=df['Volume']).ease_of_movement()\n",
    "    df['Volume_Price_Trend'] = ta.volume.VolumePriceTrendIndicator(close=df['Close'], volume=df['Volume']).volume_price_trend()\n",
    "    df['VWAP'] = ta.volume.VolumeWeightedAveragePrice(high=df['High'], low=df['Low'], close=df['Close'], volume=df['Volume']).volume_weighted_average_price()\n",
    "    \n",
    "    try:\n",
    "        df['SMA_200'] = ta.trend.SMAIndicator(close=df['Close'], window=200).sma_indicator()\n",
    "        df['EMA_200'] = ta.trend.EMAIndicator(close=df['Close'], window=200).ema_indicator()\n",
    "        df['ADX'] = ta.trend.ADXIndicator(high=df['High'], low=df['Low'], close=df['Close']).adx()\n",
    "        df['Vortex_Pos'] = ta.trend.VortexIndicator(high=df['High'], low=df['Low'], close=df['Close']).vortex_indicator_pos()\n",
    "        df['Vortex_Neg'] = ta.trend.VortexIndicator(high=df['High'], low=df['Low'], close=df['Close']).vortex_indicator_neg()\n",
    "    except ValueError as e:\n",
    "        print(f\"Could not calculate some indicators due to insufficient data: {e}\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fa60b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_close_prices(generator, noise_dim, num_samples):\n",
    "    noise = torch.randn(num_samples, noise_dim, device=device)\n",
    "    synthetic_close_prices = generator(noise).detach().cpu().numpy().flatten()\n",
    "    return synthetic_close_prices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1506fec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def derive_high_low_close(close_prices):\n",
    "    high_prices = close_prices * 1.02  # Assume High = Close + 2% of Close\n",
    "    low_prices = close_prices * 0.98  # Assume Low = Close - 2% of Close\n",
    "    return high_prices, low_prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca45194",
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_to_company_name = {\n",
    "    \"MSUMI.NS\": \"Motherson Sumi Systems Ltd.\",\n",
    "    \"TORNTPOWER.NS\": \"Torrent Power Ltd.\",\n",
    "    \"GODREJPROP.NS\": \"Godrej Properties Ltd.\",\n",
    "    \"SRF.NS\": \"SRF Ltd.\",\n",
    "    \"APLAPOLLO.NS\": \"APL Apollo Tubes Ltd.\",\n",
    "    \"TVSMOTOR.NS\": \"TVS Motor Company Ltd.\",\n",
    "    \"PAGEIND.NS\": \"Page Industries Ltd.\",\n",
    "    \"AUROPHARMA.NS\": \"Aurobindo Pharma Ltd.\",\n",
    "    \"JINDALSTEL.NS\": \"Jindal Steel & Power Ltd.\",\n",
    "    \"BAJAJHLDNG.NS\": \"Bajaj Holdings & Investment Ltd.\",\n",
    "    \"BATAINDIA.NS\": \"Bata India Ltd.\",\n",
    "    \"BHEL.NS\": \"Bharat Heavy Electricals Ltd.\",\n",
    "    \"CANBK.NS\": \"Canara Bank\",\n",
    "    \"CHOLAFIN.NS\": \"Cholamandalam Investment and Finance Company Ltd.\",\n",
    "    \"CUB.NS\": \"City Union Bank Ltd.\",\n",
    "    \"DALMIASUG.NS\": \"Dalmia Bharat Sugar and Industries Ltd.\",\n",
    "    \"ESCORTS.NS\": \"Escorts Ltd.\",\n",
    "    \"FEDERALBNK.NS\": \"The Federal Bank Ltd.\",\n",
    "    \"FORTIS.NS\": \"Fortis Healthcare Ltd.\",\n",
    "    \"GICRE.NS\": \"General Insurance Corporation of India\",\n",
    "    \"GMRINFRA.NS\": \"GMR Infrastructure Ltd.\",\n",
    "    \"GNFC.NS\": \"Gujarat Narmada Valley Fertilizers & Chemicals Ltd.\",\n",
    "    \"GODREJAGRO.NS\": \"Godrej Agrovet Ltd.\",\n",
    "    \"GRASIM.NS\": \"Grasim Industries Ltd.\",\n",
    "    \"HAVELLS.NS\": \"Havells India Ltd.\",\n",
    "    \"HINDPETRO.NS\": \"Hindustan Petroleum Corporation Ltd.\",\n",
    "    \"INDHOTEL.NS\": \"The Indian Hotels Company Ltd.\",\n",
    "    \"JUBLFOOD.NS\": \"Jubilant FoodWorks Ltd.\",\n",
    "    \"LICHSGFIN.NS\": \"LIC Housing Finance Ltd.\",\n",
    "    \"M&MFIN.NS\": \"Mahindra & Mahindra Financial Services Ltd.\",\n",
    "    \"MANAPPURAM.NS\": \"Manappuram Finance Ltd.\",\n",
    "    \"MRF.NS\": \"MRF Ltd.\",\n",
    "    \"NATCOPHARM.NS\": \"Natco Pharma Ltd.\",\n",
    "    \"NCC.NS\": \"NCC Ltd.\",\n",
    "    \"NMDC.NS\": \"NMDC Ltd.\",\n",
    "    \"OBEROIRLTY.NS\": \"Oberoi Realty Ltd.\",\n",
    "    \"PERSISTENT.NS\": \"Persistent Systems Ltd.\",\n",
    "    \"PETRONET.NS\": \"Petronet LNG Ltd.\",\n",
    "    \"RAMCOCEM.NS\": \"The Ramco Cements Ltd.\",\n",
    "    \"RBLBANK.NS\": \"RBL Bank Ltd.\",\n",
    "    \"SAIL.NS\": \"Steel Authority of India Ltd.\",\n",
    "    \"SUNTV.NS\": \"Sun TV Network Ltd.\",\n",
    "    \"TATACOMM.NS\": \"Tata Communications Ltd.\",\n",
    "    \"TATAPOWER.NS\": \"Tata Power Company Ltd.\",\n",
    "    \"THYROCARE.NS\": \"Thyrocare Technologies Ltd.\",\n",
    "    \"TORNTPHARM.NS\": \"Torrent Pharmaceuticals Ltd.\",\n",
    "    \"TRENT.NS\": \"Trent Ltd.\",\n",
    "    \"VOLTAS.NS\": \"Voltas Ltd.\",\n",
    "    \"WHIRLPOOL.NS\": \"Whirlpool of India Ltd.\",\n",
    "    \"YESBANK.NS\": \"Yes Bank Ltd.\",\n",
    "    \"ZEEL.NS\": \"Zee Entertainment Enterprises Ltd.\",\n",
    "    \"ZYDUSWELL.NS\": \"Zydus Wellness Ltd.\",\n",
    "    \"ABBOTINDIA.NS\": \"Abbott India Ltd.\",\n",
    "    \"ASHOKLEY.NS\": \"Ashok Leyland Ltd.\",\n",
    "    \"BALKRISIND.NS\": \"Balkrishna Industries Ltd.\",\n",
    "    \"BEL.NS\": \"Bharat Electronics Ltd.\",\n",
    "    \"CONCOR.NS\": \"Container Corporation of India Ltd.\",\n",
    "    \"CROMPTON.NS\": \"Crompton Greaves Consumer Electricals Ltd.\",\n",
    "    \"DEEPAKNTR.NS\": \"Deepak Nitrite Ltd.\",\n",
    "    \"DIXON.NS\": \"Dixon Technologies (India) Ltd.\",\n",
    "    \"EMAMILTD.NS\": \"Emami Ltd.\",\n",
    "    \"INDIAMART.NS\": \"IndiaMART InterMESH Ltd.\",\n",
    "    \"IRCTC.NS\": \"Indian Railway Catering and Tourism Corporation Ltd.\",\n",
    "    \"JUBLPHARMA.NS\": \"Jubilant Pharmova Ltd.\",\n",
    "    \"LTTS.NS\": \"L&T Technology Services Ltd.\",\n",
    "    \"MFSL.NS\": \"Max Financial Services Ltd.\",\n",
    "    \"METROPOLIS.NS\": \"Metropolis Healthcare Ltd.\",\n",
    "    \"OBEROIRLTY.NS\": \"Oberoi Realty Ltd.\",\n",
    "    \"PIIND.NS\": \"PI Industries Ltd.\",\n",
    "    \"POLYCAB.NS\": \"Polycab India Ltd.\",\n",
    "    \"RECLTD.NS\": \"REC Ltd.\",\n",
    "    \"SUPREMEIND.NS\": \"Supreme Industries Ltd.\",\n",
    "    \"TATACONSUM.NS\": \"Tata Consumer Products Ltd.\",\n",
    "    \"TV18BRDCST.NS\": \"TV18 Broadcast Ltd.\",\n",
    "    \"VGUARD.NS\": \"V-Guard Industries Ltd.\",\n",
    "    \"VBL.NS\": \"Varun Beverages Ltd.\",\n",
    "    \"VINATIORGA.NS\": \"Vinati Organics Ltd.\",\n",
    "    \"ZENSARTECH.NS\": \"Zensar Technologies Ltd.\",\n",
    "    \"IDFCFIRSTB.NS\": \"IDFC First Bank Ltd.\",\n",
    "    \"SONACOMS.NS\": \"Sona BLW Precision Forgings Ltd.\",\n",
    "    \"AMBUJACEM.NS\": \"Ambuja Cements Ltd.\",\n",
    "    \"GAIL.NS\": \"GAIL (India) Ltd.\",\n",
    "    \"TATAELXSI.NS\": \"Tata Elxsi Ltd.\",\n",
    "    \"MAXHEALTH.NS\": \"Max Healthcare Institute Ltd.\",\n",
    "    \"LALPATHLAB.NS\": \"Dr. Lal PathLabs Ltd.\",\n",
    "    \"JSWENERGY.NS\": \"JSW Energy Ltd.\",\n",
    "    \"AARTIIND.NS\": \"Aarti Industries Ltd.\",\n",
    "    \"ADANIGREEN.NS\": \"Adani Green Energy Ltd.\",\n",
    "    \"ABFRL.NS\": \"Aditya Birla Fashion and Retail Ltd.\",\n",
    "    \"BANDHANBNK.NS\": \"Bandhan Bank Ltd.\",\n",
    "    \"BANKINDIA.NS\": \"Bank of India\",\n",
    "    \"BERGEPAINT.NS\": \"Berger Paints India Ltd.\",\n",
    "    \"BOSCHLTD.NS\": \"Bosch Ltd.\",\n",
    "    \"CUMMINSIND.NS\": \"Cummins India Ltd.\",\n",
    "    \"DMART.NS\": \"Avenue Supermarts Ltd.\",\n",
    "    \"GLENMARK.NS\": \"Glenmark Pharmaceuticals Ltd.\",\n",
    "    \"GUJGASLTD.NS\": \"Gujarat Gas Ltd.\",\n",
    "    \"HAL.NS\": \"Hindustan Aeronautics Ltd.\",\n",
    "    \"LICI.NS\": \"Life Insurance Corporation of India\",\n",
    "    \"LUXIND.NS\": \"Lux Industries Ltd.\",\n",
    "    \"NAUKRI.NS\": \"Info Edge (India) Ltd.\",\n",
    "    \"PHOENIXLTD.NS\": \"The Phoenix Mills Ltd.\",\n",
    "    \"RAJESHEXPO.NS\": \"Rajesh Exports Ltd.\",\n",
    "    \"SHREECEM.NS\": \"Shree Cement Ltd.\",\n",
    "    \"TATACHEM.NS\": \"Tata Chemicals Ltd.\",\n",
    "    \"THERMAX.NS\": \"Thermax Ltd.\",\n",
    "    \"TTKPRESTIG.NS\": \"TTK Prestige Ltd.\",\n",
    "    \"UJJIVANSFB.NS\": \"Ujjivan Small Finance Bank Ltd.\",\n",
    "    \"VAKRANGEE.NS\": \"Vakrangee Ltd.\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d84c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_save_synthetic_data(generator, noise_dim, num_samples, stock_ticker, original_data, output_dir):\n",
    "    noise = torch.randn(num_samples, noise_dim).to(device)\n",
    "    synthetic_close_prices = generator(noise).cpu().detach().numpy().flatten()\n",
    "\n",
    "    # Set Open prices based on Close prices\n",
    "    synthetic_open_prices = np.roll(synthetic_close_prices, 1)\n",
    "    synthetic_open_prices[0] = synthetic_close_prices[0]  # First Open = First Close\n",
    "\n",
    "    # Derive High and Low prices\n",
    "    synthetic_high_prices, synthetic_low_prices = derive_high_low_close(synthetic_close_prices)\n",
    "\n",
    "    # Retain and pad the original volume data\n",
    "    synthetic_volume = original_data['Volume'].values[:num_samples]\n",
    "    if len(synthetic_volume) < num_samples:\n",
    "        avg_volume = np.mean(synthetic_volume)\n",
    "        synthetic_volume = np.pad(synthetic_volume, (0, num_samples - len(synthetic_volume)), 'constant', constant_values=avg_volume)\n",
    "\n",
    "    synthetic_data = {\n",
    "        'Open': synthetic_open_prices,\n",
    "        'High': synthetic_high_prices,\n",
    "        'Low': synthetic_low_prices,\n",
    "        'Close': synthetic_close_prices,\n",
    "        'Volume': synthetic_volume\n",
    "    }\n",
    "\n",
    "    final_synthetic_df = recalculate_indicators(synthetic_data)\n",
    "\n",
    "    # Save the synthetic data\n",
    "    output_dir = os.path.join(output_dir, stock_ticker)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    final_synthetic_df.to_csv(os.path.join(output_dir, f'{stock_ticker}_synthetic.csv'), index=False)\n",
    "\n",
    "    print(f\"Synthetic data for {stock_ticker} has been saved in {output_dir}/\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae35bfc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "noise_dim = 100\n",
    "num_samples = 1000\n",
    "num_epochs = 1000\n",
    "lr = 0.0002\n",
    "output_dir = 'data/synthetic_data'\n",
    "\n",
    "# Generate synthetic data for each stock\n",
    "for stock_ticker, company_name in ticker_to_company_name.items():\n",
    "    print(f\"Processing stock: {stock_ticker} ({company_name})\")\n",
    "    \n",
    "    # Load the historical data for the stock\n",
    "    stock_data_path = f'data/processed/{stock_ticker}_final.csv'\n",
    "    stock_data = pd.read_csv(stock_data_path)\n",
    "    \n",
    "    # Extract column names for saving\n",
    "    columns = stock_data.columns\n",
    "\n",
    "    # Train the GAN for this stock\n",
    "    generator = train_gan_for_stock(stock_data, num_epochs, noise_dim, lr)\n",
    "    \n",
    "    # Generate and save synthetic data for all scenarios\n",
    "    generate_and_save_synthetic_data(generator, noise_dim, num_samples, stock_ticker, stock_data , output_dir)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
